{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### Import required libraries along with the images dataset\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from  keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "\n",
    "(X_train, y_train), (X_val, y_val) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000, 10000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data stored as tuple\n",
    "len(X_train), len(y_train), len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the training datasets from pixels which are of 28* 28( multi dimensioanl arrays 28 lists/array with 28 elements each in it) to single array of len 784 elements\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_val = X_val.reshape(10000, 784)\n",
    "X_train= X_train.astype('float32')\n",
    "X_val= X_val.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double the trianing dataset using len func\n",
    "len(X_train[1])\n",
    "type(X_train[1][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data,this step  ensures that each input parameter (in this case pixel) has a similar data distribution. And values are scale b/w 0 and 1.\n",
    "# We can also use X-mean/std.dev to normalize where mean = 0, however in this case we used X- min/ max-min\n",
    "X_train = X_train/255\n",
    "X_val = X_val/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one hot encoding on the Y label\n",
    "nclass = 10\n",
    "y_train = to_categorical(y_train, nclass)\n",
    "y_val =to_categorical(y_val, nclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[3] # has number \"2\" tagged for that array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a base line neural net work  by defining the parameters \n",
    "no_epochs =20 # number of iterations \n",
    "batch_size = 256\n",
    "verbose = 1\n",
    "nclass = 10\n",
    "optimizer = SGD() # algo to reduce error(loss function)\n",
    "no_hidden = 128\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the outer  layer and build the model without any hidden network \n",
    "import tensorflow as tf \n",
    "np.random.seed(1272)\n",
    "model = Sequential() # composing model type, other type is functional \n",
    "model.add(Dense(nclass, input_shape= (784, )))\n",
    "# final layer which gives probablities b/w 0 to 1 , softmax is generalization of sigmiod function\n",
    "model.add(Activation(tf.nn.softmax)) \n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 1.6917 - acc: 0.5471 - val_loss: 1.2266 - val_acc: 0.7608\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 1.0685 - acc: 0.7794 - val_loss: 0.8912 - val_acc: 0.8218\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.8484 - acc: 0.8158 - val_loss: 0.7419 - val_acc: 0.8409\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.7366 - acc: 0.8329 - val_loss: 0.6570 - val_acc: 0.8542\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.6678 - acc: 0.8447 - val_loss: 0.6015 - val_acc: 0.8614\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.6205 - acc: 0.8527 - val_loss: 0.5624 - val_acc: 0.8661\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.5855 - acc: 0.8574 - val_loss: 0.5329 - val_acc: 0.8725\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.5585 - acc: 0.8625 - val_loss: 0.5098 - val_acc: 0.8764\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.5369 - acc: 0.8656 - val_loss: 0.4912 - val_acc: 0.8792\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.5190 - acc: 0.8690 - val_loss: 0.4759 - val_acc: 0.8813\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.5040 - acc: 0.8718 - val_loss: 0.4628 - val_acc: 0.8830\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4912 - acc: 0.8739 - val_loss: 0.4517 - val_acc: 0.8847\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4800 - acc: 0.8761 - val_loss: 0.4420 - val_acc: 0.8868\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4702 - acc: 0.8779 - val_loss: 0.4335 - val_acc: 0.8878\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.4615 - acc: 0.8793 - val_loss: 0.4259 - val_acc: 0.8899\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.4538 - acc: 0.8812 - val_loss: 0.4192 - val_acc: 0.8914\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4467 - acc: 0.8823 - val_loss: 0.4131 - val_acc: 0.8921\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.4404 - acc: 0.8835 - val_loss: 0.4076 - val_acc: 0.8930\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4346 - acc: 0.8844 - val_loss: 0.4025 - val_acc: 0.8939\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.4292 - acc: 0.8856 - val_loss: 0.3980 - val_acc: 0.8952\n"
     ]
    }
   ],
   "source": [
    "# Complie the model\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer =optimizer, metrics = ['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs = no_epochs,  verbose = verbose, validation_split = validation_split )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 35us/step\n",
      "test score is: 0.4000422086954117\n",
      "test accuracy : 0.8952\n"
     ]
    }
   ],
   "source": [
    "# Score the model on the unseen test data set and compare the accuracy metrics\n",
    "\n",
    "score = model.evaluate(X_val, y_val, verbose= verbose)\n",
    "print(\"test score is:\", score[0])\n",
    "print(\"test accuracy :\", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4000422086954117, 0.8952]\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Now try to build a model with hidden activation/hidden layers using sigmiod objective function/activation function for hidden layers\n",
    "# However the final output layer perceptron still will have the activation function of softmax to get probabilites\n",
    "# using sigmoid\n",
    "# Keras compatability \n",
    "model_sigmoid = Sequential()\n",
    "model_sigmoid.add(Dense(no_hidden, input_shape= (784, )))\n",
    "model_sigmoid.add(Activation(tf.nn.sigmoid))\n",
    "# add hidden layers of \n",
    "model_sigmoid.add(Dense(no_hidden, input_shape= (784, )))\n",
    "model_sigmoid.add(Activation(tf.nn.sigmoid))\n",
    "#ouptlayer\n",
    "model_sigmoid.add(Dense(nclass))\n",
    "model_sigmoid.add(Activation(tf.nn.softmax))\n",
    "model_sigmoid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 130,902\n",
      "Trainable params: 130,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model_sigmoid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 2.3157 - acc: 0.1139 - val_loss: 2.2932 - val_acc: 0.1065\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 2.2840 - acc: 0.1343 - val_loss: 2.2759 - val_acc: 0.1232\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 2.2664 - acc: 0.1896 - val_loss: 2.2581 - val_acc: 0.1841\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 2.2480 - acc: 0.2443 - val_loss: 2.2380 - val_acc: 0.2604\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 2.2280 - acc: 0.3111 - val_loss: 2.2158 - val_acc: 0.3740\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 2.2054 - acc: 0.3887 - val_loss: 2.1920 - val_acc: 0.3194\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 2.1799 - acc: 0.4176 - val_loss: 2.1636 - val_acc: 0.4614\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 2.1501 - acc: 0.4699 - val_loss: 2.1311 - val_acc: 0.4834\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 2.1156 - acc: 0.5015 - val_loss: 2.0931 - val_acc: 0.5185\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 2.0752 - acc: 0.5218 - val_loss: 2.0491 - val_acc: 0.5389\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 2.0282 - acc: 0.5426 - val_loss: 1.9971 - val_acc: 0.5610\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 1.9738 - acc: 0.5553 - val_loss: 1.9382 - val_acc: 0.5865\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 1.9123 - acc: 0.5734 - val_loss: 1.8723 - val_acc: 0.5934\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 1.8438 - acc: 0.5869 - val_loss: 1.7997 - val_acc: 0.6215\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 1.7700 - acc: 0.6031 - val_loss: 1.7221 - val_acc: 0.6272\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 1.6920 - acc: 0.6166 - val_loss: 1.6409 - val_acc: 0.6371\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 1.6117 - acc: 0.6305 - val_loss: 1.5585 - val_acc: 0.6608\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 1.5314 - acc: 0.6480 - val_loss: 1.4771 - val_acc: 0.6775\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 1.4527 - acc: 0.6622 - val_loss: 1.3984 - val_acc: 0.6921\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 1.3774 - acc: 0.6770 - val_loss: 1.3232 - val_acc: 0.7063\n"
     ]
    }
   ],
   "source": [
    "# Compile model using the sigmoid activation function\n",
    "\n",
    "model_sigmoid.compile(loss= 'categorical_crossentropy', optimizer =optimizer, metrics = ['accuracy'])\n",
    "history_sigmoid = model_sigmoid.fit(X_train, y_train, batch_size=batch_size, epochs = no_epochs,  verbose = verbose, validation_split = validation_split )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 47us/step\n",
      "test score is: 1.3234374908447266\n",
      "test accuracy : 0.6972\n"
     ]
    }
   ],
   "source": [
    "# we can observe that when we used sigmoid function in hidden layers the test accuracy was reduced by 20% form baseline\n",
    "\n",
    "score_sigmoid = model_sigmoid.evaluate(X_val, y_val, verbose= verbose)\n",
    "print(\"test score is:\", score_sigmoid[0])\n",
    "print(\"test accuracy :\", score_sigmoid[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# let us relu activation function and comapre with base line. \n",
    "model_relu = Sequential()\n",
    "model_relu.add(Dense(no_hidden, input_shape= (784, )))\n",
    "model_relu.add(Activation(tf.nn.relu))\n",
    "# add hidden layers of \n",
    "model_relu.add(Dense(no_hidden, input_shape= (784, )))\n",
    "model_relu.add(Activation(tf.nn.relu))\n",
    "#ouptlayer\n",
    "model_relu.add(Dense(nclass))\n",
    "model_relu.add(Activation(tf.nn.softmax))\n",
    "model_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 1.8491 - acc: 0.5247 - val_loss: 1.3142 - val_acc: 0.7513\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.9876 - acc: 0.7867 - val_loss: 0.7184 - val_acc: 0.8363\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.6442 - acc: 0.8384 - val_loss: 0.5295 - val_acc: 0.8673\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.5161 - acc: 0.8638 - val_loss: 0.4464 - val_acc: 0.8800\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.4505 - acc: 0.8780 - val_loss: 0.3994 - val_acc: 0.8922\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.4101 - acc: 0.8870 - val_loss: 0.3697 - val_acc: 0.8985\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3822 - acc: 0.8935 - val_loss: 0.3478 - val_acc: 0.9048\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.3614 - acc: 0.8978 - val_loss: 0.3319 - val_acc: 0.9076\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.3452 - acc: 0.9022 - val_loss: 0.3192 - val_acc: 0.9089\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.3319 - acc: 0.9056 - val_loss: 0.3075 - val_acc: 0.9132\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3206 - acc: 0.9091 - val_loss: 0.2984 - val_acc: 0.9148\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.3107 - acc: 0.9118 - val_loss: 0.2903 - val_acc: 0.9169\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.3021 - acc: 0.9137 - val_loss: 0.2836 - val_acc: 0.9193\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2944 - acc: 0.9162 - val_loss: 0.2772 - val_acc: 0.9195\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.2872 - acc: 0.9182 - val_loss: 0.2709 - val_acc: 0.9218\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.2807 - acc: 0.9197 - val_loss: 0.2659 - val_acc: 0.9243\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.2745 - acc: 0.9219 - val_loss: 0.2611 - val_acc: 0.9242\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.2690 - acc: 0.9232 - val_loss: 0.2568 - val_acc: 0.9257\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.2638 - acc: 0.9246 - val_loss: 0.2511 - val_acc: 0.9277\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.2586 - acc: 0.9267 - val_loss: 0.2482 - val_acc: 0.9279\n"
     ]
    }
   ],
   "source": [
    "# Compile model using the activation activation function\n",
    "\n",
    "model_relu.compile(loss= 'categorical_crossentropy', optimizer =optimizer, metrics = ['accuracy'])\n",
    "history_relu = model_relu.fit(X_train, y_train, batch_size=batch_size, epochs = no_epochs,  verbose = verbose, validation_split = validation_split )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 53us/step\n",
      "test score is: 0.2507254888683558\n",
      "test accuracy : 0.9279\n"
     ]
    }
   ],
   "source": [
    "# we can observe that when we used relu  function with two  hidden layers (128 eacg) the test accuracy increased  to 92.7% \n",
    "# adding drouput layers might increase accuracy further. \n",
    "score_relu = model_relu.evaluate(X_val, y_val, verbose= verbose)\n",
    "print(\"test score is:\", score_relu[0])\n",
    "print(\"test accuracy :\", score_relu[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sunayaka'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
